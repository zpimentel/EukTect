{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7b4b37-3220-40e0-95bd-57f71d292752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import requests\n",
    "import subprocess\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef33f1ae-b5d7-491d-8e3b-0118026e83c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ": boolean\n",
      "    use_inf_as_null had been deprecated and will be removed in a future\n",
      "    version. Use `use_inf_as_na` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpimentel/miniconda3/envs/VirTect/lib/python3.6/site-packages/pandas/_config/config.py:622: FutureWarning: \n",
      ": boolean\n",
      "    use_inf_as_null had been deprecated and will be removed in a future\n",
      "    version. Use `use_inf_as_na` instead.\n",
      "\n",
      "  warnings.warn(d.msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e811363-7a8d-4e20-a084-3998246a2387",
   "metadata": {},
   "source": [
    "Create necessary directories and download RefSeq assembly summaries and NCBI taxonomy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64b47c0-d049-47bf-8180-8b967ea092bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpimentel/miniconda3/envs/VirTect/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./genomes'):\n",
    "    os.makedirs('./genomes')\n",
    "\n",
    "def download_file(url, outfile):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(outfile, 'wb').write(r.content)\n",
    "\n",
    "def unzip_db(filename, outdir):\n",
    "    cmd = ['unzip', filename, '-d', outdir]\n",
    "    unzip_call = subprocess.call(cmd, shell=False, stderr=subprocess.STDOUT)\n",
    "\n",
    "refseq_handle = \"https://ftp.ncbi.nih.gov/genomes/refseq/assembly_summary_refseq.txt\"\n",
    "taxdmp = 'https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip'\n",
    "\n",
    "download_file(refseq_handle, 'assembly_summary_refseq.txt')\n",
    "download_file(taxdmp, 'taxdmp.zip')\n",
    "unzip_db('taxdmp.zip', 'taxonomy')\n",
    "\n",
    "df = pd.read_csv(refseq_handle, skiprows=1, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a4924-fdea-42dd-a763-dcad66831632",
   "metadata": {},
   "source": [
    "Parse nodes.dmp from NCBI taxonomy (map each taxid to its parent taxid) and build a function that will be used to classify every genome from the RefSeq assembly summary as eukaryotic, bacteria, archaea, or virus based on taxid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621adcab-5930-4d9a-abeb-d4a198a549fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {}\n",
    "nodes_file = open(\"./taxonomy/nodes.dmp\")\n",
    "for row in nodes_file:\n",
    "    row = row.strip().split('\\t|\\t')\n",
    "    child = row[0]\n",
    "    parent = row[1]\n",
    "    node_dict[child] = parent\n",
    "    \n",
    "\n",
    "def assign_genomes(taxid, node_dict = node_dict):\n",
    "    \"\"\"\n",
    "    Given a NCBI taxonomy ID, this function figures out the domain that ID belongs to.\n",
    "    \"\"\"\n",
    "    eukaryote = 2759\n",
    "    bacteria = 2\n",
    "    archaea = 2157\n",
    "    virus = 10239\n",
    "    \n",
    "    taxa_list = [eukaryote, bacteria, archaea, virus]\n",
    "    classification = \"not classified\"\n",
    "        \n",
    "    while taxid not in taxa_list:\n",
    "        try:\n",
    "            taxid = int(node_dict[str(taxid)])\n",
    "            return(assign_genomes(taxid, node_dict)) # get the parent taxid and try again\n",
    "        except:\n",
    "            return(classification)\n",
    "\n",
    "    if taxid == eukaryote: classification = 'eukaryote'\n",
    "    if taxid == bacteria: classification = 'bacteria'\n",
    "    if taxid == archaea: classification = 'archaea'\n",
    "    if taxid == virus: classification = 'virus'\n",
    "    \n",
    "    return(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c01442-0b57-4be5-b687-17fba95ce24f",
   "metadata": {},
   "source": [
    "Assign every genome as eukaryotic, bacteria, archaea, or virus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e606211c-a30c-458b-a377-281af5246644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Classification'] = df['taxid'].apply(assign_genomes)\n",
    "df.to_csv(\"assembly_summary_refseq_labeled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02b0b1-e506-4eb1-a60a-618a377d4eac",
   "metadata": {},
   "source": [
    "Download 10 randomly sampled eukaryotic genomes and 50 randomly sampled bacterial genomes for training. Only trying 10 euks because I am crazy and doing this on my laptop. This takes a while =("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d14d13-d6ad-4fac-a0c8-85e95e7a3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_euks = df[df['Classification'] == 'eukaryote'].sample(50, random_state=234)\n",
    "sampled_bact = df[df['Classification'] == 'bacteria'].sample(50, random_state=345)\n",
    "\n",
    "sampled_euks[sampled_euks.columns[0]] = sampled_euks[sampled_euks.columns[0]].str.split('.').str[0]\n",
    "sampled_bact[sampled_bact.columns[0]] = sampled_bact[sampled_bact.columns[0]].str.split('.').str[0]\n",
    "\n",
    "sampled_euks_list = sampled_euks[sampled_euks.columns[0]].to_list()\n",
    "sampled_bact_list = sampled_bact[sampled_bact.columns[0]].to_list()\n",
    "\n",
    "for ftp in sampled_euks['ftp_path'].to_list():\n",
    "    genome_id = ftp.split(\"/\")[-1]\n",
    "    gen_url = os.path.join(ftp,genome_id + \"_genomic.fna.gz\").replace(\" \", \"_\")\n",
    "    r = requests.get(gen_url, allow_redirects=True)\n",
    "    open(\"genomes/\" + genome_id + \".fna.gzip\", 'wb').write(r.content)\n",
    "\n",
    "for ftp in sampled_bact['ftp_path'].to_list():\n",
    "    genome_id = ftp.split(\"/\")[-1]\n",
    "    gen_url = os.path.join(ftp,genome_id + \"_genomic.fna.gz\").replace(\" \", \"_\")\n",
    "    r = requests.get(gen_url, allow_redirects=True)\n",
    "    open(\"genomes/\" + genome_id + \".fna.gzip\", 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dee890-89c1-4ab6-9728-c50a7ccc3d37",
   "metadata": {},
   "source": [
    "For each genome, up to ~1 Mb is used and is broken into non-overlapping contigs of ~50 Kb which were used to count the frequencies of canonical 5-mers computed with a sliding window (meaning each 5-mer and its reverse complement are considered the same). To reduce memory requirements and runtime, I decided to use canonical kmers, used fairly large contigs (50 Kb), and only used a portion of the eukaryotic genomes (up to 1 Mb). I use the ~ because I could be much more stringent about actually making sure everything is the same size - contigs generated from the ends of a chromosome/contig could be smaller for example. I do some QC to remove small stuff later on though. This also takes a while =("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b825f3d2-7807-47cd-9f51-b8b0836cfff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpimentel/miniconda3/envs/VirTect/lib/python3.6/site-packages/Bio/Seq.py:155: BiopythonWarning: Biopython Seq objects now use string comparison. Older versions of Biopython used object comparison. During this transition, please use hash(id(my_seq)) or my_dict[id(my_seq)] if you want the old behaviour, or use hash(str(my_seq)) or my_dict[str(my_seq)] for the new string hashing behaviour.\n",
      "  \"the new string hashing behaviour.\", BiopythonWarning)\n"
     ]
    }
   ],
   "source": [
    "def is_valid_sequence(seq):\n",
    "    ''' Used to remove any kmers with N's in them '''\n",
    "    return set(seq).issubset({\"A\", \"T\", \"C\", \"G\"})\n",
    "\n",
    "kmers_visited = set()\n",
    "kmer_dict = defaultdict(lambda:0)\n",
    "contigs = set(\" \")\n",
    "\n",
    "directory = \"./genomes\"\n",
    "euk_count = 0\n",
    "bact_count = 0\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".fna.gzip\"):\n",
    "        genome_bp_count = 0\n",
    "        \n",
    "        genome_id = \"_\".join(filename.split(\"_\")[0:2]).split('.')[0]\n",
    "        \n",
    "        if genome_id in sampled_euks_list: genome_type = 'euk'\n",
    "        elif genome_id in sampled_bact_list: genome_type = 'bact'\n",
    "        else: genome_type = 'NA'\n",
    "            \n",
    "        #print(genome_id, genome_type)\n",
    "        \n",
    "        with gzip.open(os.path.join(directory, filename), \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                if genome_bp_count < 1000000:\n",
    "                    if len(record.seq) >= 50000:\n",
    "                        for i in range(0, len(record.seq), 50000):\n",
    "                            genome_bp_count += 50000\n",
    "                            if genome_type == 'euk': \n",
    "                                euk_count += 1\n",
    "                                contig_5kb_name = \"euk_\" + str(euk_count)\n",
    "                            elif genome_type == 'bact': \n",
    "                                bact_count += 1\n",
    "                                contig_5kb_name = \"bact_\" + str(bact_count)\n",
    "                            contigs.add(contig_5kb_name)\n",
    "                            \n",
    "                            if genome_bp_count < 1000000:\n",
    "                                contig_5kb = record.seq[i:i+50000] \n",
    "                                for j in range(len(contig_5kb)):\n",
    "                                    kmer = contig_5kb[j:j+5].upper()\n",
    "                                    if kmer.reverse_complement() in kmers_visited:\n",
    "                                        kmer = kmer.reverse_complement()\n",
    "                                    if len(kmer) == 5:\n",
    "                                        if is_valid_sequence(kmer) is True:\n",
    "                                            kmers_visited.add(str(kmer))\n",
    "                                            kmer_dict[(contig_5kb_name, str(kmer))] += 1\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e50513-e72d-471e-ae7c-6ba20e5afef8",
   "metadata": {},
   "source": [
    "Export the counts of the 512 unique 5-mers per contig in a table so that I can jump back in here without having to re-download the genomes or count the kmers again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30abffe-c67e-4302-8acb-d2ad3190724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"kmer_matrix.tsv\", \"w\")\n",
    "\n",
    "f.write(\"Contig\" + \"\\t\")\n",
    "f.write(\"\\t\".join(contigs))\n",
    "f.write(\"\\n\")\n",
    "for kmer in kmers_visited:\n",
    "    f.write(kmer + \"\\t\")\n",
    "    for contig in contigs:\n",
    "        f.write(str(kmer_dict[(contig, kmer)]) + \"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d11fc-46df-4231-97bc-820a156f4854",
   "metadata": {},
   "source": [
    "Load the 5-mer count data per genome into a pandas dataframe, remove chunks with less than 10,000 kmers, and transform the counts to proportions by dividing each kmer count per genome by the total number of kmers per contig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b0210da-f40c-43c1-b391-d14c0289aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_df = pd.read_csv(\"kmer_matrix.tsv\", sep=\"\\t\", index_col=False)\n",
    "kmer_df = kmer_df.set_index(\"Contig\")\n",
    "kmer_df = kmer_df.loc[:, kmer_df.sum(axis=0) > 5000]\n",
    "kmer_df = kmer_df.div(kmer_df.sum(axis=0),axis=1)\n",
    "kmer_df = kmer_df.reindex(sorted(kmers_visited))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f3fa0-af8e-418a-928e-680b2d63995a",
   "metadata": {},
   "source": [
    "Prepare data for training. Make a list of lists where each sublist contains the kmer frequncies from one column/contig. Make another list containing the dummy variable for each list (containing the kmer frequencies per contig) whether it be eukaryotic (1) or bacterial (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2dbead4-b89d-4142-9683-79410e477345",
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_kmers = []\n",
    "for column in kmer_df:\n",
    "    kmer_list = kmer_df[column].tolist()\n",
    "    contig_kmers.append(kmer_list)\n",
    "    \n",
    "prelabels = kmer_df.columns.tolist()\n",
    "labels = []\n",
    "for i in prelabels:\n",
    "    if i.split(\"_\")[0] == \"euk\":\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        \n",
    "with open('kmer_list.pkl', 'wb') as f:\n",
    "    pickle.dump(kmers_visited, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ec155-a861-485f-9941-af71447e02ef",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff460356-6243-4d17-a0ae-a9b1d7d1160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(contig_kmers, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4be8a6-557f-409a-8b8b-e8cba28d3889",
   "metadata": {},
   "source": [
    "Train the SVM and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1c6561e-098f-4314-9358-d43416e67ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "model_outfile = 'kmer_SVM.sav'\n",
    "pickle.dump(clf, open(model_outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cedba8-f38a-4a33-83cd-c49e06670728",
   "metadata": {},
   "source": [
    "Assess the accuracy of the model using the testing data. Need to remember that although these test contigs were not included in the training data, they are from the same genomes used in the training set. Therefore, additional validation is needed to assess accuracy on genomes the model has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52661190-313d-4f18-87a9-35f7f46cfb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the testing set is 99.56222639149468%\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print(f\"Accuracy in the testing set is {accuracy_score(y_test, predictions) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165e693-1aef-43b6-9cb7-570f7def28b1",
   "metadata": {},
   "source": [
    "Things to improve: Rather than using randomly selected eukaryotic and bacterial genomes for training select a set of taxonomically diverse genomes, make more efforts to make sure there is an even amount of bacterial and euk data in the training data, include a separate validation set of unseen genomes. Specifically, I know the model performs poorly on GC-rich bacterial genomes which weren't seen in the training set, so inclusion of some of those could help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e06f25-f195-428d-afe5-a3788b85bddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
